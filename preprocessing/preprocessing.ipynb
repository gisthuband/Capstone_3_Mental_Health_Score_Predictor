{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d06c118d",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "\n",
    "This is the third installment of the Capstone 3 project: Preprocessing\n",
    "Here are links to the other installments:\n",
    "\n",
    "Data Wrangle https://github.com/gisthuband/Capstone_3_Mental_Health_Score_Predictor/blob/main/data_wrangle/data_wrangle.ipynb\n",
    "\n",
    "Exploratory Data Analysis https://github.com/gisthuband/Capstone_3_Mental_Health_Score_Predictor/blob/main/exploratory_data_analysis/exploratory_data_analysis.ipynb\n",
    "\n",
    "\n",
    "\n",
    "This project will be used to predict the mental health scores of individuals based on race/ethnicity, the unemployment rate change, and the unemployment change.\n",
    "\n",
    "Inputs: Race/Ethnicity, unemployment rate change, unemployment change\n",
    "\n",
    "Outputs: Mental Health Score\n",
    "\n",
    "In this notebook I aim to come up with a preprocessing object that can be used in the modeling step of this project.\n",
    "\n",
    "What will the object be able to generate:\n",
    "\n",
    "1.) A dataframe with only numeric information\n",
    "\n",
    "2.) Numeric Information with a constant (for OLS)\n",
    "\n",
    "3.) Numeric Information standardized\n",
    "\n",
    "4.) A dataframe with a one hot encoding of categorical variables\n",
    "\n",
    "5.) One hot encoded with a constant (for OLS, Lasso, Ridge)\n",
    "\n",
    "6.) One hot encoded standardized\n",
    "\n",
    "7.) A numeric only dataframe that has random values\n",
    "\n",
    "8.) A one hot encoded dataframe that has random values\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f09910c5",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5b865f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a9fdb4d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('explored_data_v1.csv')\n",
    "df =df.drop(columns='Unnamed: 0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ff31f475",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Indicator</th>\n",
       "      <th>Subgroup</th>\n",
       "      <th>Date</th>\n",
       "      <th>Value</th>\n",
       "      <th>Year</th>\n",
       "      <th>weighted_unemployment_rate_change</th>\n",
       "      <th>total_unemployed_change</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Symptoms of Depressive Disorder</td>\n",
       "      <td>Hispanic or Latino</td>\n",
       "      <td>2020-04-23</td>\n",
       "      <td>29.4</td>\n",
       "      <td>2020</td>\n",
       "      <td>6.168662</td>\n",
       "      <td>1771000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Symptoms of Anxiety Disorder</td>\n",
       "      <td>Hispanic or Latino</td>\n",
       "      <td>2020-04-23</td>\n",
       "      <td>36.3</td>\n",
       "      <td>2020</td>\n",
       "      <td>6.168662</td>\n",
       "      <td>1771000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Symptoms of Anxiety Disorder or Depressive Dis...</td>\n",
       "      <td>Hispanic or Latino</td>\n",
       "      <td>2020-04-23</td>\n",
       "      <td>42.7</td>\n",
       "      <td>2020</td>\n",
       "      <td>6.168662</td>\n",
       "      <td>1771000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Symptoms of Depressive Disorder</td>\n",
       "      <td>Hispanic or Latino</td>\n",
       "      <td>2020-05-07</td>\n",
       "      <td>27.9</td>\n",
       "      <td>2020</td>\n",
       "      <td>6.168662</td>\n",
       "      <td>1771000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Symptoms of Anxiety Disorder</td>\n",
       "      <td>Hispanic or Latino</td>\n",
       "      <td>2020-05-07</td>\n",
       "      <td>36.2</td>\n",
       "      <td>2020</td>\n",
       "      <td>6.168662</td>\n",
       "      <td>1771000.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           Indicator            Subgroup  \\\n",
       "0                    Symptoms of Depressive Disorder  Hispanic or Latino   \n",
       "1                       Symptoms of Anxiety Disorder  Hispanic or Latino   \n",
       "2  Symptoms of Anxiety Disorder or Depressive Dis...  Hispanic or Latino   \n",
       "3                    Symptoms of Depressive Disorder  Hispanic or Latino   \n",
       "4                       Symptoms of Anxiety Disorder  Hispanic or Latino   \n",
       "\n",
       "         Date  Value  Year  weighted_unemployment_rate_change  \\\n",
       "0  2020-04-23   29.4  2020                           6.168662   \n",
       "1  2020-04-23   36.3  2020                           6.168662   \n",
       "2  2020-04-23   42.7  2020                           6.168662   \n",
       "3  2020-05-07   27.9  2020                           6.168662   \n",
       "4  2020-05-07   36.2  2020                           6.168662   \n",
       "\n",
       "   total_unemployed_change  \n",
       "0                1771000.0  \n",
       "1                1771000.0  \n",
       "2                1771000.0  \n",
       "3                1771000.0  \n",
       "4                1771000.0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "51352233",
   "metadata": {},
   "outputs": [],
   "source": [
    "class preprocess:\n",
    "    def __init__(self, df_path):\n",
    "        self.df_path = df_path\n",
    "    \n",
    "    #1\n",
    "    def numeric_only(df_path):\n",
    "        \n",
    "        df = pd.read_csv(df_path)\n",
    "\n",
    "        df = df.drop(columns='Unnamed: 0')\n",
    "\n",
    "        df = df.drop(columns=['Date','Year','Indicator','Subgroup'])\n",
    "        \n",
    "        features = list(df.columns[df.columns != 'Value'])\n",
    "\n",
    "        X = df[features]\n",
    "        \n",
    "        y = df['Value']\n",
    "        \n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2, random_state=5)\n",
    "        \n",
    "        return X_train, X_test, y_train, y_test\n",
    "    \n",
    "    \n",
    "    #2\n",
    "    def numeric_with_constant(df_path):\n",
    "        \n",
    "        df = pd.read_csv(df_path)\n",
    "\n",
    "        df = df.drop(columns='Unnamed: 0')\n",
    "\n",
    "        df = df.drop(columns=['Date','Year','Indicator','Subgroup'])\n",
    "        \n",
    "        features = list(df.columns[df.columns != 'Value'])\n",
    "\n",
    "        X = df[features]\n",
    "        X = sm.add_constant(X)\n",
    "        \n",
    "        y = df['Value']\n",
    "        \n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2, random_state=5)\n",
    "        \n",
    "        return X_train, X_test, y_train, y_test\n",
    "    \n",
    "    #3\n",
    "    def numeric_standardized(df_path):\n",
    "        \n",
    "        df = pd.read_csv(df_path)\n",
    "\n",
    "        df = df.drop(columns='Unnamed: 0')\n",
    "\n",
    "        df = df.drop(columns=['Date','Year','Indicator','Subgroup'])\n",
    "        \n",
    "        features = list(df.columns[df.columns != 'Value'])\n",
    "\n",
    "        X = df[features]\n",
    "        \n",
    "        y = df['Value']\n",
    "        \n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2, random_state=13)\n",
    "        \n",
    "        s_scaler = preprocessing.StandardScaler().fit(X_train)\n",
    "        \n",
    "        X_train=s_scaler.transform(X_train)\n",
    "        \n",
    "        X_test=s_scaler.transform(X_test)\n",
    "        \n",
    "        return X_train, X_test, y_train, y_test\n",
    "    \n",
    "    #4\n",
    "    def dummied(df_path):\n",
    "        \n",
    "        df = pd.read_csv(df_path)\n",
    "\n",
    "        df = df.drop(columns='Unnamed: 0')\n",
    "        \n",
    "        df = df.drop(columns=['Date','Year'])\n",
    "        \n",
    "        dum_df = pd.get_dummies(df[['Indicator','Subgroup']])        \n",
    "        \n",
    "        dummed_df = pd.concat([df, dum_df],axis=1)\n",
    "\n",
    "        dummed_df = dummed_df.drop(columns=['Indicator','Subgroup'])\n",
    "        \n",
    "        features = list(dummed_df.columns[dummed_df.columns != 'Value'])\n",
    "        \n",
    "        X = dummed_df[features]\n",
    "\n",
    "        y = dummed_df['Value']\n",
    "\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2)\n",
    "        \n",
    "        return X_train, X_test, y_train, y_test\n",
    "    #5\n",
    "    def dummied_with_constant(df_path):\n",
    "        \n",
    "        df = pd.read_csv(df_path)\n",
    "\n",
    "        df = df.drop(columns='Unnamed: 0')\n",
    "        \n",
    "        df = df.drop(columns=['Date','Year'])\n",
    "        \n",
    "        dum_df = pd.get_dummies(df[['Indicator','Subgroup']])        \n",
    "        \n",
    "        dummed_df = pd.concat([df, dum_df],axis=1)\n",
    "\n",
    "        dummed_df = dummed_df.drop(columns=['Indicator','Subgroup'])\n",
    "        \n",
    "        features = list(dummed_df.columns[dummed_df.columns != 'Value'])\n",
    "        \n",
    "        X = dummed_df[features]\n",
    "        X = sm.add_constant(X)\n",
    "\n",
    "        y = dummed_df['Value']\n",
    "\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2)\n",
    "        \n",
    "        return X_train, X_test, y_train, y_test\n",
    "    \n",
    "    #6\n",
    "    def dummied_standardized(df_path):\n",
    "        \n",
    "        df = pd.read_csv(df_path)\n",
    "\n",
    "        df = df.drop(columns='Unnamed: 0')\n",
    "        \n",
    "        df = df.drop(columns=['Date','Year'])\n",
    "        \n",
    "        dum_df = pd.get_dummies(df[['Indicator','Subgroup']])        \n",
    "        \n",
    "        dummed_df = pd.concat([df, dum_df],axis=1)\n",
    "\n",
    "        dummed_df = dummed_df.drop(columns=['Indicator','Subgroup'])\n",
    "        \n",
    "        features = list(dummed_df.columns[dummed_df.columns != 'Value'])\n",
    "        \n",
    "        X = dummed_df[features]\n",
    "\n",
    "        y = dummed_df['Value']\n",
    "\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2)\n",
    "        \n",
    "        s_scaler = preprocessing.StandardScaler().fit(X_train)\n",
    "        \n",
    "        X_train=s_scaler.transform(X_train)\n",
    "        \n",
    "        X_test=s_scaler.transform(X_test)\n",
    "        \n",
    "        return X_train, X_test, y_train, y_test\n",
    "        \n",
    "    \n",
    "    #7\n",
    "    def random_label_numeric_only(df_path):\n",
    "        \n",
    "        df = pd.read_csv(df_path)\n",
    "\n",
    "        df = df.drop(columns='Unnamed: 0')\n",
    "\n",
    "        df = df.drop(columns=['Date','Year','Indicator','Subgroup'])\n",
    "        \n",
    "        rng = np.random.default_rng()\n",
    "        \n",
    "        rand_scores = list(rng.integers(low=0, high=48, size=len(df))) \n",
    "        \n",
    "        df['random_values'] = rand_scores\n",
    "        \n",
    "        df = df.drop(columns='Value')\n",
    "        \n",
    "        features = list(df.columns[df.columns != 'random_values'])\n",
    "\n",
    "        X = df[features]\n",
    "        \n",
    "        y = df['random_values']\n",
    "        \n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2, random_state=6)\n",
    "        \n",
    "        return X_train, X_test, y_train, y_test\n",
    "    \n",
    "    #8\n",
    "    def random_label_dum(df_path):\n",
    "        \n",
    "        df = pd.read_csv(df_path)\n",
    "\n",
    "        df = df.drop(columns='Unnamed: 0')\n",
    "        \n",
    "        df = df.drop(columns=['Date','Year'])\n",
    "        \n",
    "        dum_df = pd.get_dummies(df[['Indicator','Subgroup']])        \n",
    "        \n",
    "        dummed_df = pd.concat([df, dum_df],axis=1)\n",
    "\n",
    "        dummed_df = dummed_df.drop(columns=['Indicator','Subgroup'])\n",
    "        \n",
    "        rng = np.random.default_rng()\n",
    "        \n",
    "        rand_scores = list(rng.integers(low=0, high=48, size=len(df))) \n",
    "        \n",
    "        dummed_df['random_values'] = rand_scores\n",
    "        \n",
    "        dummed_df = dummed_df.drop(columns='Value')\n",
    "        \n",
    "        features = list(dummed_df.columns[dummed_df.columns != 'random_values'])\n",
    "\n",
    "        X = dummed_df[features]\n",
    "        \n",
    "        y = dummed_df['random_values']\n",
    "        \n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2, random_state=9)\n",
    "        \n",
    "        return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3e2fcce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "trial_real = preprocess.dummied('explored_data_v1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1bfde139",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(556, 9)\n",
      "(140, 9)\n",
      "(556,)\n",
      "(140,)\n"
     ]
    }
   ],
   "source": [
    "for x in range(4):\n",
    "    print (trial_real[x].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "08503d7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "78     29.5\n",
       "83     39.6\n",
       "690    22.3\n",
       "222    23.3\n",
       "154    25.9\n",
       "       ... \n",
       "298    26.6\n",
       "29     43.4\n",
       "151    29.1\n",
       "163    31.6\n",
       "97     30.2\n",
       "Name: Value, Length: 556, dtype: float64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trial_real[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "78c084d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "trial_fake =preprocess.random_label_dum('explored_data_v1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1e75d2df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(556, 9)\n",
      "(140, 9)\n",
      "(556,)\n",
      "(140,)\n"
     ]
    }
   ],
   "source": [
    "for x in range(4):\n",
    "    print (trial_fake[x].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4feb7307",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "88     27\n",
       "21     18\n",
       "608     6\n",
       "276    45\n",
       "504    13\n",
       "       ..\n",
       "56     31\n",
       "501     0\n",
       "638    14\n",
       "348     0\n",
       "382     9\n",
       "Name: random_values, Length: 556, dtype: int64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trial_fake[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8d24eb32",
   "metadata": {},
   "outputs": [],
   "source": [
    "trial = preprocess.dummied_with_constant('explored_data_v1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "13734dd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "const                                                            float64\n",
      "weighted_unemployment_rate_change                                float64\n",
      "total_unemployed_change                                          float64\n",
      "Indicator_Symptoms of Anxiety Disorder                              bool\n",
      "Indicator_Symptoms of Anxiety Disorder or Depressive Disorder       bool\n",
      "Indicator_Symptoms of Depressive Disorder                           bool\n",
      "Subgroup_Hispanic or Latino                                         bool\n",
      "Subgroup_Non-Hispanic Asian, single race                            bool\n",
      "Subgroup_Non-Hispanic Black, single race                            bool\n",
      "Subgroup_Non-Hispanic White, single race                            bool\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print (trial[0].dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a277884",
   "metadata": {},
   "source": [
    "## Summary:\n",
    "\n",
    "This notebook contains the class that will be used to train the models in the next step.\n",
    "\n",
    "There is now a dataframe containing only originally numeric information.\n",
    "\n",
    "Also there is now a dataframe containing one hot encoding of the categorical information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d937711",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
